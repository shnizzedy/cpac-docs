# this is a C-PAC Docs content file 

paragraphs:

  - paragraph: Introduction
    subparagraphs:
      - paragraph: What is C-PAC?
        details:
          - The Configurable Pipeline for the Analysis of Connectomes (C-PAC) is an open-source, modular software pipeline designed to facilitate the analysis of functional and structural brain connectivity using MRI data. Built on top of the Nipype framework, C-PAC offers researchers a flexible and scalable environment for preprocessing, quality assurance, and advanced statistical analysis of neuroimaging data. It supports a wide range of configurable workflows, including motion correction, spatial normalization, nuisance regression, and network modeling, allowing users to tailor the pipeline to their specific research needs. C-PAC is particularly valuable for large-scale studies and reproducible neuroscience, as it integrates with standardized data formats like BIDS and promotes transparency in neuroimaging analysis workflows.
      - paragraph: C-PAC vs cpac
        details:
          - C-PAC (Configurable Pipeline for the Analysis of Connectomes) is a robust, modular neuroimaging workflow system built on Nipype, designed to support reproducible and scalable analysis of resting-state and task-based fMRI data. It integrates tools from major neuroimaging packages such as AFNI, FSL, and ANTs, offering a comprehensive suite of preprocessing steps including slice timing correction, motion correction, spatial normalization, nuisance regression, and parcellation-based time series extraction. C-PAC is highly configurable via YAML files, supports the Brain Imaging Data Structure (BIDS), and can be deployed in containerized environments for reproducibility. It also supports group-level analyses using model specifications and mixed-effects modeling.
          - The cpac Python package available on PyPI is a lightweight Python interface that provides programmatic access to some C-PAC functionalities and configuration management, serving more as a utility for scripting or integration within Python workflows. Python packages like cpac are distributed as installable modules via the Python Package Index (PyPI), allowing users to integrate specific components of larger software systems into their custom pipelines without running the full C-PAC application. 
          - While both share the same ecosystem, the full C-PAC pipeline is designed for end-to-end data processing, whereas the cpac package facilitates modular use and development within the Python programming environment.

  - paragraph: Ways to Run
    subparagraphs:
      - paragraph: Container
        details:
          - Docker - Utilize the official Docker image fcpindi/c-pac to run C-PAC on your local machine. This method ensures a consistent environment and simplifies installation.
          - Singularity/Apptainer - Ideal for HPC environments where Docker may not be available. You can convert the Docker image to a Singularity image or use pre-built Singularity images to run C-PAC.
          - cpac Python package - Install the cpac package via pip to manage and run C-PAC containers. This wrapper simplifies execution and allows for additional functionalities like running extra packages (ba_timeseries_gradients, tsconcat).
      - paragraph: Shelling Into a Container
        details:
        - Shelling into a container allows you to interactively run C-PAC commands within the container environment.  This is useful for debugging, testing configurations, or running ad-hoc analyses.
        - To shell into a Docker container, use docker run -it fcpindi/c-pac /bin/bash.
        - For Singularity, use singularity shell cpac_latest.sif.
        - Once inside the container, you can run C-PAC commands as if you were in a regular terminal environment.
        - This method provides full access to the C-PAC command-line interface and allows you to explore the container's file system and installed tools.
        - Remember to mount relevant directories (data, output) when launching the container interactively. 
      - paragraph: High Performance Computing (HPC)
        details:
          - Singularity - Leverage Singularity containers to run C-PAC on HPC systems, ensuring compatibility and ease of deployment.
          - Starcluster - Use Starcluster to set up and manage HPC clusters on AWS or other HPC environments. This setup allows for parallel processing of multiple subjects, enhancing computational efficiency.
      - paragraph: Cloud Platforms
        details:
          - AWS - Deploy C-PAC using the official Amazon Machine Image (AMI) available on the AWS Marketplace. This option supports both single-instance runs and scalable HPC clusters using Starcluster.
          - OpenNeuro - Run C-PAC analyses directly through the OpenNeuro platform, which provides a web interface for processing public neuroimaging datasets without local installation.
          - brainlife.io -  Utilize brainlife.io to run C-PAC pipelines in the cloud, benefiting from its user-friendly interface and integration with various neuroimaging tools.
          - Flywheel -  Integrate C-PAC into the FlyWheel platform to manage and process neuroimaging data within a comprehensive data management system.

  - paragraph: C-PAC Quickstart
    subparagraphs:
      - paragraph: Installing and Launching
        details:
          - null
      - paragraph: First-Time Run Overview
        details:
          - null
      - paragraph: Troubleshooting Setup
        details:
          - null

  - paragraph: Specify Your Data
    subparagraphs:
      - paragraph: BIDS Compliance
        details:
          - C-PAC expects your input data to follow the BIDS (Brain Imaging Data Structure) format, which standardizes how neuroimaging data is organized.
          - BIDS organizes data by subject and session folders, with standardized filenames and metadata files (.json, .tsv).
          - BIDS ensures smooth pipeline execution and compatibility with other tools.
          - Use tools like BIDS Validator (https://bids-standard.github.io/bids-validator/) to verify your dataset's compliance before running C-PAC.
          - Example directory structure for subject 01.
        codeblocks:
          - |
            ├── dataset
            ├── sub-01
            │   ├── anat
            │   │   └── sub-01_T1w.nii.gz
            │   └── func
            │       └── sub-01_task-rest_bold.nii.gz
      - paragraph: Custom Data Formats
        details:
          - If your data does not follow BIDS, you can still run C-PAC by specifying a subject list configuration file (YAML or JSON) to manually point C-PAC to your data locations.
          - The subject list config file defines the paths to anatomical (T1), functional (bold), and other modalities for each subject.
          - This flexibility lets you process datasets that don't conform to BIDS without reorganizing files.  However, you must ensure paths and filenames are accurate and consistent across your config.
      - paragraph: Common Data Formatting Pitfalls
        details:
          - Incorrect path references - Paths in the subject list config must be absolute or relative to the config file location; wrong paths cause errors.
          - Misnaming files or folders-  Mismatched filenames or extensions (e.g., .nii vs .nii.gz) will cause failures.
          - Missing required data types - Some C-PAC pipelines require both anatomical and functional scans; missing either leads to errors.
          - Improper subject ID matching - Subject IDs in your config file should match your folder or file naming scheme exactly (case-sensitive).
          - Inconsistent sessions - If using multi-session data, ensure sessions are clearly specified and consistent in both file structure and config.
          - Ignoring BIDS metadata files - Lack of .json sidecars or event files can lead to incomplete preprocessing or QC issues.
          - Spaces or special characters in paths - These may cause command-line parsing errors; avoid or properly escape them.

  - paragraph: Select Your Pipeline
    subparagraphs:
      - paragraph: Default Pipelines
        details:
          - C-PAC comes with several default pipeline configurations, each tailored for common use cases and analysis strategies.
          - These pipelines can be found here https://fcp-indi.github.io/cpac-docs/pages/pipelines.html, and in the Docker image or GitHub repo under resources/config/pipeline.
          - These pipelines are useful starting points and can be run as-is or used as templates for custom pipelines.
          - To use a default pipeline -
        codeblocks:
          - cpac run --pipeline_file /path/to/default_pipeline.yml ...
      - paragraph: Custom Pipelines
        details:
          - You can create a custom pipeline by modifying a default pipeline YAML or building one from scratch using C-PAC’s pipeline structure.
          - Custom pipelines allow you to enable/disable specific steps like skull-stripping, registration, nuisance regression, etc., swap tools (e.g., use FSL instead of ANTs for registration)., and adjust parameters such as smoothing kernel size, motion threshold, or temporal filtering band.
          - to create a custom pipeline, copy the default pipeline config YAML and modify the relevant sections under the appropraite keys.  Save and reference your modified file using --pipeline_file.
          - Example modification changing the functional smoothing kernel -
        codeblocks:
          - |
            ├── functional_preproc:
            │   ├── functional_smoothing:
            │   │   └── fwhm: 6
      - paragraph: Important and Modifying Pipelines
        details:
          - Pipelines are defined in YAML configuration files, which follow a modular, nested structure.
          - To modify a pipeline, focus on key processing modules, like registration_workflows ad nuisance_regression.
          - You can define multiple pipelines in one run using the pipeline_setup / pipeline_name / key.
          - Use include_config in your YAML to import modular settings from another file
          - Always validate your edited pipeline with a test subject before launching a full run.
          - Use include_config in your YAML to import modular settings from another file -
        codeblocks:
          - |
            ├── include_config:
            │   ├── /path/to/base_pipeline.yml
  - paragraph: Design a Pipeline
    subparagraphs:
      - paragraph: Visual Pipeline Designer
        details:
          - C-PAC includes a Visual Pipeline Builder (VBP) web interface to help users create and edit pipeline YAMLs without manual editing.
          - It provides a point-and-click GUI to enable/disable pipeline steps, set key parameters (e.g., spatial smoothing FWHM, temporal filter bands), and choose tool options (e.g., FSL vs. ANTs for registration).
          - The VPB is accessible via the web browser using cpac gui, or inside Docker/Singularity using appropriate port forwarding.
          - After configuring the pipeline, you can export the YAML file, which can then be used with the cpac run command.
          - Using the VPB helps to avoid common YAML syntax errors and makes it easier for new users to get started.
      - paragraph: Forking Pipelines
        details:
          - C-PAC allows you to fork pipelines within a single run configuration, enabling multiple pipelines to process the same data in parallel.
          - Useful for comparing preprocessing strategies or parameter choices (e.g., with and without global signal regression).
          - Forking reduces re-computation by reusing cached intermediate results where possible.
          - Example use case - test effect of different nuisance regression models across the same dataset.
          - Define forks under pipeline_setup in your YAML file -
        codeblocks:
          - |
              pipeline_setup:
                pipeline_1:
                  # config A
                pipeline_2:
                  # config B
      - paragraph: Parameter Tweaks
        details:
          - All configurable parameters in C-PAC pipelines are exposed in the YAML.
          - Parameters are grouped by processing modules, such as anatomical_preproc.
          - You can fine-tune parameters to suit your dataset’s quality or your scientific questions.
          - It's best practice to document each version of your pipeline YAML to ensure reproducibility and clarity in group comparisons.
          - Example: Disable slice-timing correction -
        codeblocks:
          - |
              functional_preproc:
                slice_timing_correction:
                  enabled: false

  - paragraph: Pre-Process Your Data
    subparagraphs:
      - paragraph: Overview of Preprocessing Steps
        details:
          - null
      - paragraph: Selecting and Modifying Preprocessing Modules
        details:
          - null

  - paragraph: All Run Options
    subparagraphs:
      - paragraph: Command Line Execution
        details:
          - C-PAC can be launched via the command line using the cpac command.
          - Basic command structure -
        codeblocks:
          - |
            cpac run --pipeline_file /path/to/pipeline.yml \
            --subject_list /path/to/subject_list.yml \
            --output_dir /path/to/output_directory
          - pipeline_file - specifies the YAML configuration for the preprocessing pipeline
          - subject_list file - defines the input data paths for each subject
          - output_dir - where all processed data and logs will be saved
          - additional options include --working_dir to specify a temporary working directory, --n_procs to set parallel processing threads, and --skip_existing to skip subjects with completed outputs
      - paragraph: YAML Configuration 
        details:
          - C-PAC relies heavily on YAML configuration files to manage workflows.  Two key config files are used during C-PAC runs -
          - Pipeline config YAML - defines all preprocessing and analysis st options like motion correction, template registration, and analysis outputs (e.g., ALFF, ReHo, FC).
          - Subject list YAML - specifies the location of anatomical and functional data, with each entry including a subject ID and paths to data.
          - Use the cpac utils command to generate templates -
        codeblocks:
          - |
            cpac utils generate_pipeline_config --output pipeline_config.yml
            cpac utils generate_subject_list --output subject_list.yml
      - paragraph: Warm Restart
        details:
          - Warm restart allows you to resume a previously interrupted or partially completed run without reprocessing finished steps.  This is particularly useful when working with HPC jobs that were killed early due to time/resource limits or when debugging a pipeline that failed partway through. 
          - C-PAC keeps track of completed steps via .pkl files and output directories.
          - When restarting a run, C-PAC checks for existing outputs and skips any completed steps.
          - To trigger a warm restart, simply re-run the same cpac command with the same config and output directory.  You can delete or rename output directories to force full reprocessing if needed.

  - paragraph: Run Group Analysis
    subparagraphs:
      - paragraph: Setting Up Group-Level Comparisons
        details:
          - Group-level analyses in C-PAC are performed after preprocessing and individual-level derivatives are complete.
          - Use the group command-line tool within the C-PAC container to run statistical comparisons across subjects.
          - Required inputs are (1) a list of subject-level derivative files (e.g., ALFF maps, ReHo maps) and (2) a phenotype or design matrix file (.csv) containing covariates and group labels.
          - Common workflow- gather the derivative of interest for all subjects -> reate a design matrix CSV (columns = variables, rows = subjects) -> run the appropriate cpac group subcommand (e.g., feat or feat-load).
          - Outputs include group stattistical maps, contrast maps, and logs.
          - Example command -
        codeblocks:
          - |
            singularity exec cpac_latest.sif cpac group feat \
            --data_file subject_alff_paths.txt \
            --design_file design_matrix.csv \
            --output_dir /output/group_analysis
      - paragraph: Paired and Unpaired Designs
        details:
          - C-PAC supports both paired and unpaired group comparison designs.
          - Use the appropriate group command variant.  
          - group feat for unpaired design (e.g., comparing two independent groups).
          - group feat-load-paired for  paired design (e.g., pre/post treatment within subjects).
          - In unpaired design, each subject is treated independently, which is useful for between-group comparisons (like control vs. patient groups).
          - In paired design, Subjects appear in both conditions (e.g., baseline and follow-up).  The design matrix should reflect pairing by consistent subject IDs or ordering.
          - Be sure to match the order of subjects and control for subject-specific variability in paired designs.
          - Example for paired design -
        codeblocks:
          - |
            cpac group feat-load-paired \
              --group1_files group1_alff_paths.txt \
              --group2_files group2_alff_paths.txt \
              --output_dir /output/group_paired
      - paragraph: Handling Multiple Sessions
        details:
          - If subjects have multiple sessions (e.g., test/retest, baseline/follow-up), you can run group analyses in a few ways.  Either trreat sessions as repeated measures (paired analysis), run separate group comparisons for each session, or collapse sessions if appropriate (e.g., average across sessions).
          - You must manually organize session-specific files and label them accordingly in your subject list or data paths.
          - You are responsible for ensuring consistent file ordering and accurate session labeling across design and data files.
          - Always double-check your data structure before running group stats to avoid mismatched inputs.
          - Example structure for group1_files.txt -
        codeblocks:
          - |
            /output/sub-01/ses-01/alff.nii.gz
            /output/sub-02/ses-01/alff.nii.gz

  - paragraph: Check Your Outputs
    subparagraphs:
      - paragraph: Output Structure
        details:
          - After running C-PAC, outputs are organized inside the output_directory you specify in the config or command line.
          - The directory structure is hierarchical, grouped by subject_id (e.g., sub-01), session_id (if applicable), and pipeline_name (e.g., default_pipeline)
          - Inside each pipeline folder, you’ll find subfolders for each preprocessing step and derivative (e.g., func_preproc, regression, derivatives).
          - A summary CSV file (summary.csv) is often generated containing key QC and processing metrics across subjects.
          - Example path for a subject’s preprocessed functional data -
        codeblocks:
          - output_directory/sub-01/pipeline_name/func_preproc/sub-01_task-rest_bold_preproc.nii.gz
      - paragraph: Interpreting Results
        details:
          - Each output file corresponds to a specific processing step or derivative (e.g., motion-corrected images, normalized data, connectivity matrices).
          - File naming convention includes subject, task, and processing step for clarity (e.g., sub-01_task-rest_bold_regressors.tsv).
          - The logs directory contains detailed logs for each subject and pipeline step, useful for troubleshooting errors or unexpected results.
          - Use the output directory to verify processing completeness and data integrity before downstream analyses.
          - Inspect CSV summary files for metrics like framewise displacement, number of censored volumes, and data quality indicators.
      - paragraph: Quality Control Reports
        details:
          - C-PAC automatically generates QC reports as HTML or PDF files inside the subject’s output folder, often under qc or reports.
          - QC reports typically include visualizations of motion correction parameters, time series plots, registration quality images (e.g., alignment of functional to anatomical scans), summary statistics on signal-to-noise ratio (SNR), and other quality metrics.
          - Reviewing QC reports helps identify data or processing issues early (e.g., excessive motion, misalignment).
          - Reports provide links or thumbnails to key images for quick visual inspection.  Use these reports to decide whether to exclude or reprocess problematic subjects.

  - paragraph: Debugging Tools
    subparagraphs:
      - paragraph: Understanding Error Logs
        details:
          - C-PAC generates log files during runtime that capture detailed information about each pipeline step.
          - Logs (pypeline.log) are located in the log/ directory within the output directory or next to the working directory.
          - Error logs include timestamps, module names, and full Python tracebacks, which are key for identifying where and why the pipeline failed.
          - Look for lines marked ERROR or CRITICAL.
          - Trace back through the preceding lines to see which step and module caused the failure.
          - Example log -
        codeblocks:
          - 2025-05-30 12:34:56,789 - CPAC.func_preproc - ERROR - File not found /sub-01/func/rest_bold.nii.gz
      - paragraph: Common Runtime Issues
        details:
          - Missing or misnamed input files are the cause of most common errors, especially when using a custom subject list.  Ensure full paths are valid and filenames match exactly.
          - Incorrect working directory permissions will cause C-PAC to fail if it cannot write to the working or output directory.  Check ownership and permissions using ls -la and chmod/chown if needed.
          - Docker/Singularity volume binding errors are also common.  Make sure host directories are correctly mounted inside the container and you are using the correct container image for the version of cpac you want to use.
          - C-PAC is resource-intensive, so failures may occur if insufficient RAM, disk, or swap space is available.
          - For source installs, Python version/package mismatches can happen.  Using a virtual environment or a container is recommended to prevent dependency conflicts.
          - Indentation or character errors in your config files can silently cause pipeline misconfigurations.  Use a YAML linter or syntax checker to ensure proper YAML formatting.
    